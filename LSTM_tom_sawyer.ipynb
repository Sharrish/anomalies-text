{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_tom_sawyer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIQOw47g0PAN"
      },
      "source": [
        "# Генерация текста с помощью LSTM рекуррентных нейронных сетей в Python с Keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjrM5MZCsssE"
      },
      "source": [
        "# Ипорт необходимых библиотек\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sjIDO5-zh7N"
      },
      "source": [
        "## Подготовка даных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz1z1ohMtxMe"
      },
      "source": [
        "# Загрузка текста и приведение к нижнему регистру\n",
        "filename = \"tom_sawyer.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cQ-H5zkufl5"
      },
      "source": [
        "# Подготовка данных\n",
        "# Создание словаря уникальных символов в числа\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUwj4oKwviBz",
        "outputId": "ac7fae92-8da0-47f7-9020-abe50e299370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Сведения о датасете\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  393551\n",
            "Total Vocab:  51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dM5o_Xe0DIi"
      },
      "source": [
        "Будем идти по тексту окном в 100 символов. Тогда каждый экземпляр входных данных будет состоять из 100 признаков X и иметь метку из одного символа Y - следующий, 101-й символ за ними. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8-hvbXgv9gd",
        "outputId": "889ed858-fe98-4487-948e-e857c121c57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# подготовка данных на вход\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(n_chars - seq_length):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[c] for c in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total training patterns: {}\".format(n_patterns))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training patterns: 393451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTUvSnuK0B9s"
      },
      "source": [
        "## Трансформация данных в приемлимый вид для Keras и LSTM в частности"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prdSb9Uc0RL-"
      },
      "source": [
        "**Во-первых**, мы должны трансформировать наш список входных данных в форму, приемлимую для LSTM.  \n",
        "\n",
        "Входные данные в LSTM долны быть трехмерные:  \n",
        "**Образцы.** Одна последовательность - это один образец. Batch     принимает один или больше образцов.  \n",
        "**Временные шаги.** Один временной шаг - это одна точка наблюдений в выборке.  \n",
        "**Признаки.** Один признак - это наблюдение за временной шаге.\n",
        "\n",
        "То есть LSTM ожидает на входе 3D-array c размерностями [samples, time steps, features]\n",
        "\n",
        "**Во-вторых**, нам нужно привести числа в диапазон [0, 1], чтобы сделать образцы легче обучаемы для LSTM, использующей сигмоидную функцию активации по умолчанию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clTuGzAgzpz6"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# нормализация к диапазону [0, 1]\n",
        "X = X / float(n_vocab)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNHQO2EB4ZrE"
      },
      "source": [
        "**В-третьих**, нам нужно конвертировать выходные метки в one-hot encoding. (т.е. все классы, которым не принадлежим 0, а для текущего класса 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCGYt3Xi4TkP"
      },
      "source": [
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnM3_99R40PY"
      },
      "source": [
        "## Определение LSTM модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0agm5aDk6MM6"
      },
      "source": [
        "Мы определим один скрытый слой LSTM с 256 блоков памяти.  \n",
        "Используем dropout с вероятностью 20%.  \n",
        "Выходной слой - плотный слой с функцией автивации softmax, для выводы вероятности в диапазоне [0, 1] для каждого из символа словаря.  \n",
        "Так как задача сводится к классификации по символам словаря, то функция потеря loss будет categorical_crossentropy, мы не используем classification accuracy, так как нам важна обобщенность, и мы не хотим переобучится.  \n",
        "Алгоритм оптимизации ADAM для скорости."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-muCemEa4xbx"
      },
      "source": [
        "# определение LSTM модели\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp87pCa3DMZI"
      },
      "source": [
        "Будем использовать checkpoint для сохранения весов сети, при улучшении loss. Будем использовать лучший набор весов для генераивной модели в будущем."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsC4_GP87p06"
      },
      "source": [
        "# Определение checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3vjw9cDEayE"
      },
      "source": [
        "## Тренировка LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJtKsGQ-EhHN"
      },
      "source": [
        "Будем использовать 20 эпох и 128 экземпляров в batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhPYRhmzD_OT",
        "outputId": "180c0787-f16e-43b6-b021-8c42bea2164c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 2.0513\n",
            "Epoch 00001: loss improved from 2.06380 to 2.05126, saving model to weights-improvement-01-2.0513.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 2.0513\n",
            "Epoch 2/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 2.0393\n",
            "Epoch 00002: loss improved from 2.05126 to 2.03933, saving model to weights-improvement-02-2.0393.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 2.0393\n",
            "Epoch 3/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 2.0275\n",
            "Epoch 00003: loss improved from 2.03933 to 2.02749, saving model to weights-improvement-03-2.0275.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 2.0275\n",
            "Epoch 4/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 2.0173\n",
            "Epoch 00004: loss improved from 2.02749 to 2.01731, saving model to weights-improvement-04-2.0173.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 2.0173\n",
            "Epoch 5/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 2.0077\n",
            "Epoch 00005: loss improved from 2.01731 to 2.00775, saving model to weights-improvement-05-2.0078.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 2.0078\n",
            "Epoch 6/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 1.9975\n",
            "Epoch 00006: loss improved from 2.00775 to 1.99752, saving model to weights-improvement-06-1.9975.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9975\n",
            "Epoch 7/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.9904\n",
            "Epoch 00007: loss improved from 1.99752 to 1.99036, saving model to weights-improvement-07-1.9904.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9904\n",
            "Epoch 8/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 1.9819\n",
            "Epoch 00008: loss improved from 1.99036 to 1.98185, saving model to weights-improvement-08-1.9818.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9818\n",
            "Epoch 9/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.9735\n",
            "Epoch 00009: loss improved from 1.98185 to 1.97356, saving model to weights-improvement-09-1.9736.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9736\n",
            "Epoch 10/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.9658\n",
            "Epoch 00010: loss improved from 1.97356 to 1.96576, saving model to weights-improvement-10-1.9658.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9658\n",
            "Epoch 11/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 1.9582\n",
            "Epoch 00011: loss improved from 1.96576 to 1.95812, saving model to weights-improvement-11-1.9581.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9581\n",
            "Epoch 12/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 1.9543\n",
            "Epoch 00012: loss improved from 1.95812 to 1.95437, saving model to weights-improvement-12-1.9544.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9544\n",
            "Epoch 13/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.9479\n",
            "Epoch 00013: loss improved from 1.95437 to 1.94790, saving model to weights-improvement-13-1.9479.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9479\n",
            "Epoch 14/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.9412\n",
            "Epoch 00014: loss improved from 1.94790 to 1.94116, saving model to weights-improvement-14-1.9412.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9412\n",
            "Epoch 15/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.9342\n",
            "Epoch 00015: loss improved from 1.94116 to 1.93421, saving model to weights-improvement-15-1.9342.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9342\n",
            "Epoch 16/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.9320\n",
            "Epoch 00016: loss improved from 1.93421 to 1.93195, saving model to weights-improvement-16-1.9320.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9320\n",
            "Epoch 17/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.9241\n",
            "Epoch 00017: loss improved from 1.93195 to 1.92409, saving model to weights-improvement-17-1.9241.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9241\n",
            "Epoch 18/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 1.9218\n",
            "Epoch 00018: loss improved from 1.92409 to 1.92180, saving model to weights-improvement-18-1.9218.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9218\n",
            "Epoch 19/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.9160\n",
            "Epoch 00019: loss improved from 1.92180 to 1.91595, saving model to weights-improvement-19-1.9160.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9160\n",
            "Epoch 20/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 1.9127\n",
            "Epoch 00020: loss improved from 1.91595 to 1.91265, saving model to weights-improvement-20-1.9126.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9126\n",
            "Epoch 21/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 1.9071\n",
            "Epoch 00021: loss improved from 1.91265 to 1.90717, saving model to weights-improvement-21-1.9072.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9072\n",
            "Epoch 22/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.9021\n",
            "Epoch 00022: loss improved from 1.90717 to 1.90215, saving model to weights-improvement-22-1.9022.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.9022\n",
            "Epoch 23/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8996\n",
            "Epoch 00023: loss improved from 1.90215 to 1.89954, saving model to weights-improvement-23-1.8995.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8995\n",
            "Epoch 24/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8968\n",
            "Epoch 00024: loss improved from 1.89954 to 1.89682, saving model to weights-improvement-24-1.8968.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8968\n",
            "Epoch 25/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8950\n",
            "Epoch 00025: loss improved from 1.89682 to 1.89488, saving model to weights-improvement-25-1.8949.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8949\n",
            "Epoch 26/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8904\n",
            "Epoch 00026: loss improved from 1.89488 to 1.89043, saving model to weights-improvement-26-1.8904.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8904\n",
            "Epoch 27/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8858\n",
            "Epoch 00027: loss improved from 1.89043 to 1.88585, saving model to weights-improvement-27-1.8859.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8859\n",
            "Epoch 28/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8814\n",
            "Epoch 00028: loss improved from 1.88585 to 1.88145, saving model to weights-improvement-28-1.8814.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8814\n",
            "Epoch 29/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 1.8776\n",
            "Epoch 00029: loss improved from 1.88145 to 1.87758, saving model to weights-improvement-29-1.8776.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8776\n",
            "Epoch 30/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 1.8738\n",
            "Epoch 00030: loss improved from 1.87758 to 1.87376, saving model to weights-improvement-30-1.8738.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8738\n",
            "Epoch 31/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8707\n",
            "Epoch 00031: loss improved from 1.87376 to 1.87067, saving model to weights-improvement-31-1.8707.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8707\n",
            "Epoch 32/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 1.8676\n",
            "Epoch 00032: loss improved from 1.87067 to 1.86760, saving model to weights-improvement-32-1.8676.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8676\n",
            "Epoch 33/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 1.8617\n",
            "Epoch 00033: loss improved from 1.86760 to 1.86172, saving model to weights-improvement-33-1.8617.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8617\n",
            "Epoch 34/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.8641\n",
            "Epoch 00034: loss did not improve from 1.86172\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8641\n",
            "Epoch 35/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.8598\n",
            "Epoch 00035: loss improved from 1.86172 to 1.85979, saving model to weights-improvement-35-1.8598.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8598\n",
            "Epoch 36/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8567\n",
            "Epoch 00036: loss improved from 1.85979 to 1.85674, saving model to weights-improvement-36-1.8567.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8567\n",
            "Epoch 37/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8543\n",
            "Epoch 00037: loss improved from 1.85674 to 1.85435, saving model to weights-improvement-37-1.8543.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8543\n",
            "Epoch 38/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 1.8527\n",
            "Epoch 00038: loss improved from 1.85435 to 1.85272, saving model to weights-improvement-38-1.8527.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8527\n",
            "Epoch 39/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.8501\n",
            "Epoch 00039: loss improved from 1.85272 to 1.85008, saving model to weights-improvement-39-1.8501.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8501\n",
            "Epoch 40/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8482\n",
            "Epoch 00040: loss improved from 1.85008 to 1.84812, saving model to weights-improvement-40-1.8481.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8481\n",
            "Epoch 41/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.8493\n",
            "Epoch 00041: loss did not improve from 1.84812\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8493\n",
            "Epoch 42/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8423\n",
            "Epoch 00042: loss improved from 1.84812 to 1.84233, saving model to weights-improvement-42-1.8423.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8423\n",
            "Epoch 43/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.8402\n",
            "Epoch 00043: loss improved from 1.84233 to 1.84018, saving model to weights-improvement-43-1.8402.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8402\n",
            "Epoch 44/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 1.8370\n",
            "Epoch 00044: loss improved from 1.84018 to 1.83702, saving model to weights-improvement-44-1.8370.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8370\n",
            "Epoch 45/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8381\n",
            "Epoch 00045: loss did not improve from 1.83702\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8382\n",
            "Epoch 46/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 1.8328\n",
            "Epoch 00046: loss improved from 1.83702 to 1.83284, saving model to weights-improvement-46-1.8328.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8328\n",
            "Epoch 47/50\n",
            "3074/3074 [==============================] - ETA: 0s - loss: 1.8310\n",
            "Epoch 00047: loss improved from 1.83284 to 1.83095, saving model to weights-improvement-47-1.8310.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8310\n",
            "Epoch 48/50\n",
            "3071/3074 [============================>.] - ETA: 0s - loss: 1.8270\n",
            "Epoch 00048: loss improved from 1.83095 to 1.82679, saving model to weights-improvement-48-1.8268.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8268\n",
            "Epoch 49/50\n",
            "3073/3074 [============================>.] - ETA: 0s - loss: 1.8272\n",
            "Epoch 00049: loss did not improve from 1.82679\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8272\n",
            "Epoch 50/50\n",
            "3072/3074 [============================>.] - ETA: 0s - loss: 1.8224\n",
            "Epoch 00050: loss improved from 1.82679 to 1.82241, saving model to weights-improvement-50-1.8224.hdf5\n",
            "3074/3074 [==============================] - 43s 14ms/step - loss: 1.8224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f92300556d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcg5WwXXhSSp"
      },
      "source": [
        "## Генерация текста с LSTM сетью"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ28eTdMjtdZ"
      },
      "source": [
        "Загружаем веса с checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMMH3aOXEyVc"
      },
      "source": [
        "# загрузка весов\n",
        "filename = \"weights-improvement-50-1.8224.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd8Hqg9HlNFy"
      },
      "source": [
        "Создание обратного словаря из чисел в символы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiLk4vPvlDuQ"
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvumhUhZnKTv"
      },
      "source": [
        "Самый простой способ генерировать последовательности текста с помощью LSTM это начать с произвольной последовательности, генерировать символ, прибавлять её к этой последовательности и отрезать у неё первый символ. Так можно сделать сколько угодно длинную сгенерированную последовательность."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4ikOoMQlW0t",
        "outputId": "0f11b2e6-73af-456c-a0c0-3eeb6cd17ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Выбор случайной последовательности\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed: {}\".format(''.join([int_to_char[value] for value in pattern])))\n",
        "# генерация символов\n",
        "for i in range(1000): # генерируем последовательность из 1000 символов\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tprint(result, end='')\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: ance, and shoutings and pistol-shots sent their\n",
            "hollow reverberations to the ear down the sombre ais\n",
            "eds of the siaee of the siee and she tiong thieg hes head and the sioes whth the seleoberen that was toon the seared toon the boys would never be aoneer thet was tomn the seared toon the boys and she pioete of the siee and the sioes and she pirete tas the siarehee that had been a groan aayenture of the seared toon the sioes and saed they were sooe tome the seared toon the siarehe of the sieee of the sieee of the siee and she sioe the seirol oo the siee and the sioe of the siarehen aadk the siarehen aedone the seared and the sioes sfat he was ao ond saake that he was an ond saake that he was ao ond saake that he was an ond saake that he was ao ond saake that he was an ond saake that he was ao ond saake that he was an ond saake that he was ao ond saake that he was an ond saake that he was ao ond saake that he was an ond saake that he was ao ond saake that he was an ond saake that he was ao ond saake that he was an ond saake that he was ao ond saake that he was an ond saake that he was ao\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-zDWQJopB7s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}